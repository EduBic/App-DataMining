\documentclass[a4paper]{article}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english, italian]{babel}

\usepackage{amsmath}

\begin{document}
	
	\section{Sommario Lezioni}
	
		\subsection{Modello lineare semplice}
		
			\subsubsection{Valutazione del modello}
				Stimato un modello, quanto si comporta bene? Per rispondere a questa domanda dobbiamo avere un modello:
				\begin{itemize}
					\item buono nel training set;
					\item buono per prevedere al di fuori del training set (test set).
				\end{itemize}
				
				Gli strumenti a nostra disposizione sono:
				\begin{itemize}
					\item Valutazioni numeriche;
					\item Valutazioni grafiche.
				\end{itemize}
				
				\paragraph{Indice di determinazione $R^2$} (valutazione numerica)
				\begin{itemize}
					\item $0 \leq R^2 \leq 1$;
					\item più $\hat{y}_i \approx y_i$ più $R^2$ tende a 1;
				\end{itemize}
				
				\subparagraph{Interpretare $R^2$}
				\begin{itemize}
					\item $R^2 > 90\%$ allora modello buono nel training set;
					\item $R^2 < 90\%$ modello mediocre;
				\end{itemize}
				
				\subparagraph{Problematiche di $R^2$ (???)}
				 Un $R^2$ molto elevato seppur indicante un modello buono può essere sintomo di qualcosa di errato nel modello. RSS deve avere un valore basso per un buon $R^2$ mentre RSE deve essere un valore piccolo perché il modello si adatti bene.
				 
				
				\paragraph{Analisi dei residui}
				
					\begin{enumerate}
						\item \textbf{Grafico dei residui}: un buon modello presenta residui senza andamenti deterministici, la distribuzione casuale di essi è l'ideale. Molto spesso in questa analisi si vedone delle \textbf{osservazioni anomale}, esse sono valori rilevati molto lontani dagli altri e possono invalidare enormemente il modello; possono essere di due tipologie:
						\begin{itemize}
							\item \textbf{outlier} non impattano nel modello;
							\item \textbf{leva} impattano nel modello.
						\end{itemize}
						Le osservazioni anomale sono evidenziate da R e possono quindi essere valutate singolarmente.
						
						\item \textbf{Previsione e intervallo di previsione}: si calcola un intervallo di confidenza per costruire introrno alla retta di regressione le \textbf{bande dell'intervallo di confidenza}, l'intervallo indica la percentuale dei valori osservati che dovrebbe stare all'interno delle bande.
					\end{enumerate}
					
					
	
	
	\section{Linear Regression}
		
		\subsection{Simple Linear Regression}
			It is an approach for predicting a quantitative response $Y$ on the basis of a single predictor variable $X$.
			
			\begin{equation}
				Y \approx \beta_0 + \beta_1 X
			\end{equation}
		
			\begin{equation}
				\hat{y} = \hat{\beta_0} + \hat{\beta_1}x				
			\end{equation}
			
			where $\hat{y}$ indicates a prediction of $Y$ on the basis of $X=x$.
		
			\subsubsection{Estimated the coefficients}
				Let $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i$:			
				\paragraph{\textit{ith} residual}
				\begin{equation}
					e_i = y_i - \hat{y}_i 
				\end{equation}
				
				\paragraph{Residual sum of squares}
				\begin{equation}\label{eq:RSS}
					RSS = e_1^2 + e_2^2 + \dots + e_n^2 = \sum_{i=1}^n e_i^2
				\end{equation}		
				
				\paragraph{Least square for $\beta_0$ and $\beta_1$}
				\begin{description}
					\item $\beta_0$: expected value of $Y$ when $X = 0$.
					\item $\beta_1$: the average increase in $Y$ associated with a one-unit increase in $X$.		
				\end{description}
				
				\begin{align}
						\hat{\beta_1} ={}& \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n(x_i - \bar{x})^2},
				\\
						\hat{\beta_0} ={}& \bar{y} - \hat{\beta}_1 \bar{x}
				\end{align}
				
			\subsubsection{Accuracy of the coefficients estimates}
				
				\paragraph{Standard error}
				\begin{align}
					SE^2(\hat{\beta_1}) ={}& \sigma^2 \Bigg[ \frac{1}{n} + \frac{\bar{x}^2}{\sum_{i=1}^{n}(x-i - \bar{x})^2} \Bigg], 
					\\
					SE^2(\hat{\beta_0}) ={}& \frac{\sigma^2}{\sum_{i=1}^n(x_i - \bar{x})^2}
				\end{align}		
				
				Where $\sigma^2 = Var(\epsilon)$. In general $\sigma$ (standard deviation) is unknown, but can be estimated from the data with the calculation of the \textit{residual standard error}.
				
				\paragraph{Residual standard error}
				\begin{equation}
					RSE = \sqrt{\frac{RSS}{n - 2}}
				\end{equation}		
				
				\paragraph{Confidence intervals}
					A 95\% confidence interval is defined as a range of values such that with 95\% probability, the range will contain the true unknown value of the parameter.
					For linear regression:
					\begin{equation}
						\hat{\beta}_1 \pm 2 \cdot SE(\hat{\beta}_1), \quad \quad
						\hat{\beta}_0 \pm 2 \cdot SE(\hat{\beta}_0)						
					\end{equation}
					
					
					\textbf{N.B.} This mean that for $\beta_1$ there is approximately 95\% chance that the interval:
					\begin{align*}
						\Big[ \hat{\beta}_1 - 2 \cdot SE(\hat{\beta}_1), \quad \hat{\beta}_1 + 2 \cdot SE(\hat{\beta}_1) \Big]
					\end{align*}																			will contain the true value of $\beta_1$.
						
			\subsubsection{Accuracy of the model}	
				
				\paragraph{Residual standard error}	
					RSE is an estimate of standard deviation of $\epsilon$. Hence it is the average amount that response will deviate from the true regression line.
				\begin{equation}\label{eq:RSE}
					RSE = \sqrt{\frac{RSS}{n - 2}} = \sqrt{\frac{1}{n - 2} \sum_{i=1}^n (y_i - \hat{y}_i)^2}
					%\tag{\ref{eq:RSE}}
				\end{equation}
				The RSE is considered a measure if the lack of fit of the model:
				\begin{itemize}
					\item if $\hat{y}_i \approx y_i$ for $i = 1,\dots, n$ the model fits the data very well (RSE small).
					\item if $\hat{y}_i$ is very far from $y_i$ for one or more observation the model doesn't fit the data well (RSE quite large).
				\end{itemize}
								
				\paragraph{$R^2$ statistic}
					$R^2$ measures the proportion of variability in Y that can be explained using X. It always lies between 0 and 1.
				\begin{equation}
					R^2 = \frac{TSS - RSS}{TSS} = 1 - \frac{RSS}{TSS}
				\end{equation}
				
				where \textit{total sum of squares} (TSS) measures the total variance in the response Y, it is the amount of variability in the response before the regression is performed.:
				\begin{equation}
					TSS = \sum(y_i - \bar{y})^2
				\end{equation}
				
				\begin{itemize}
					\item if $R^2$ is close to 1 indicates that a large proportion of the variability in the response has been explained by the regression.
					\item if $R^2$ is near 0 indicates that the regression did not explain much of the variability in the response.
				\end{itemize}
				
				\paragraph{Correlation} It is a measure of the linear relationship between X and Y like $R^2$. In the simple linear regression $R^2 = r^2$.
				\begin{equation}
					r = Cor(X,Y) = \frac{\sum_{i=1}^n (x_i - \bar{x}) (y_i - \bar{y})}{\sqrt{ \sum_{i=1}^n (x_i - \bar{x})^2 } \sqrt{ \sum_{i=1}^n (y_i - \bar{y})^2}}
				\end{equation}
				
			\subsection{Multiple Linear Regression}
				It is the extension of simple linear regression model so that it can directly accommodate multiple predictors ($p$ predictors).
				\begin{equation}
					Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p + \epsilon
				\end{equation}
				\begin{equation}
					\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x_1 + \hat{\beta}_2 x_2 + ... + \hat{\beta}_p x_p
				\end{equation}
				
				where $\beta_j$ (\textit{j}th predictor) is the average effect on Y of a one unit increase in $X_j$.
				
				\subsubsection{Estimating coefficients}
					The multiple regression coefficient estimates have somewhat complicated forms that are most easily represented using matrix algebra. The book do not provide them but use formula of language R.					
					
					\paragraph{Residual sum of squares}
					\begin{equation}
					\begin{split}
						RSS ={} & \sum_{i=1}^n (y_i - \hat{y}_i)^2 \\
							={} & \sum_{i=1}^n (y_i - \hat{\beta}_0 + \hat{\beta}_1 x_i1 + \hat{\beta}_2 x_i2 + ... + \hat{\beta}_p x_ip )^2
					\end{split}
					\end{equation}
					
				\subsubsection{Understand Multiple Linear Regression}
					
					\paragraph{Relationship response and predictors}
					In order to determine if there is a relationship between response and predictors we can check if $\beta_1=\beta_2=\dots=\beta_p = 0$. We test the \textit{null hypothesis}:
					\begin{equation}
						H_0 : \beta_1 = \beta_2 = \dots = \beta_p = 0
					\end{equation}
					\begin{equation}
						H_\alpha : \text{at least one } \beta_j \text{ is non-zero}
					\end{equation}
					
					\subparagraph{F-statistic}
					\begin{equation}
						F = \frac{(TSS - RSS)/p}{RSS / (n - p - 1)}
					\end{equation}
					where $TSS = \sum_{i=1}^n (y_i - \bar{y})^2$ and $RSS = \sum_{i=1}^n (y_i - \hat{y}_i)^2$.
					
					\begin{itemize}
						\item When there is no relationship between the response and the predictors F-statistic takes on a value close to 1.
						\item If $H_\alpha$ is true we expected F-statistic to be greater than 1.
					\end{itemize}
					
					Hence a larger F-statistic suggests that at least one of the predictor must be related to response.
						\begin{itemize}
							\item If $n$ is large and F-statistic is just a little larger than 1 might still provide evidence against $H_0$. ($n$ large $\land$ $F > 1 \Rightarrow$ there is a relation)
							\item If $n$ is small we need a larger F-static. ($n$ small $\land$ $F \gg 1 \Rightarrow$ there is a relation)
						\end{itemize}															
					\paragraph{Deciding on Important Variables}
						\textit{Variable selection} is the task od determining which predictors are associated with the response, in order to fit a single model involving only those predictors.
						There are a total of $2^p$ models that contains subset of $p$ variables. Unless $p$ is very small we cannot consider all models. There are 3 classical approaches:
						\begin{itemize}
							\item \textbf{Forward selection} begin with the \textit{null model} we then add the varaible that results in the lowest RSS for the new two-variable model.
							\item \textbf{Backward selection} (only if $p > n$) start with all variable in the model and remove the variable with the largest p-value.
							\item \textbf{Mixed selection} start with no variable in the model, we add the variable that provides the best fit. If at any point the p-value for one of the variables rises above certain threshold, then we remove that variable.
						\end{itemize}
						
						\paragraph{Model Fit}
							The common numerical measures of model fit are the RSE and $R^2$.
							
							An $R^2$ value close to 1 indicates that the model explains a large portion of the variance in the response variable. $R^2$ will always increase when more variables are added to the model, even if those variables are only weakly associated with the response. In fact if we add a new variable and this leads to a tiny increase in $R^2$ this could be an additional evidence that that variable can be dropped from the model, otherwise it can cause overfitting.
							
							When we add a new variable and it leads to a reduce in RSE, we have increased the accuracy of the model. Remember that model with model with more variables can have higher RSE if the decrease in RSS is small relative to the increase in p (see \ref{eq:RSE}).
							
						\paragraph{Predictions}
							We can compute a \textit{confidence interval} in order to determine how close $\hat{Y}$ will be to $Y$. The response value cannot be predicted perfectly because of the random error $\epsilon$ in the model. We use \textit{prediction intervals for know how much $Y$ will vary from $\hat{Y}$}.
							
							
					\subsection{Other consideration in the Regression Model}
					
						\subsubsection{Qualitative Predictors}
							
							\paragraph{Two levels}
							
							\paragraph{More than two levels}
							
						\subsubsection{Extensions for the Linear Model}
							The standard linear regression model makes \textbf{two restrictive assumptions}. Relation between predictors and response is:
							\begin{itemize}
								\item \textbf{linear}: the change in response due to a one-unit change of a predictors is constant.
								\item \textbf{additive}: the effect of predictor are independent from other predictors.
							\end{itemize}
							
							\paragraph{Removing the Additive assumption}
								In some case some predictors are dependent from others, this is called \textbf{interaction effect}. For consider it in the model we need to add an \textbf{interaction term} that is the product of two predictor with the interaction effect. Consider this model:
								\begin{align*}
									Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \epsilon
								\end{align*}
								It becomes:
								\begin{align*}
									Y ={} & \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2 + \epsilon \quad \text{or} \\
									Y ={} & \beta_0 + (\beta_1 + \beta_3 X_2) X_1 + \beta_2 X_2 + \epsilon
								\end{align*}
								where $\beta_1 + \beta_3 X_2$ is the interaction term.
								
								If the p-value for the interaction term is low, this is a strong evidence for $\beta_3 \neq 0$. So the relationship isn't additive.	
								
								\subparagraph{Hierarchical principle} \textit{Of we include an interaction in a model, we should also include the main effects, even if the p-value associated with their coefficients are not significant.}							
								
							
							\paragraph{Accommodate Non-linear Relationship}																\textbf{Polynomial regression} is a simple way to extend the linear model to accommodate non-linear relationship.
								The simple model with one predictor becomes:
								\begin{align*}
									Y = \beta_0 + \beta_1 X + \beta_2 X^2
								\end{align*}
								It is still a linear method.
						
					\subsubsection{Potential problems}
						
						\paragraph{1. Non-linearity of the Data}
						\begin{description}
							\item[Problem:] The linear regression model assumes that there is a straight-line relationship between the predictors and the response.
							
							\item[Analysis:] Residual plots are a graphical tool for identify non-linearity;
								\begin{itemize}
									\item In \textbf{simple linear regression model} we can plot the residuals, $e_i = y_i - \hat{y_i}$, versus the predictor $x_i$.
									\item In \textbf{multiple regression model} we instead plot the residuals versus the predicted values $\hat{y_i}$.
								\end{itemize}
							If the plot exhibit a recognizable shape or pattern there is non-linearity in the data.
									
							\item[Possible solution:] If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformation of the predictors, such as $\log X, \sqrt{X}$ and $X^2$.
						\end{description}
						
						
						\paragraph{2. Correlation of Error Terms}
						\begin{description}
							\item[Problem:]	An important assumption of the linear regression model us that the error terms, $\epsilon_1, \epsilon_2, \dots, \epsilon_n$, are uncorrelated.
							
							\item[Analysis:] If there is correlation among the error terms:
							\begin{itemize}
								\item Estimated standard errors will tend to underestimate the true standard errors. 
								\item Confidence and prediction intervals will be narrower than they should be.
								\item p-values associated with the model will be lower than they should be.
							\end{itemize}
							In conclusion we may have an unwarranted sense of confidence in our model.
							
							\item[Conclusions:] In general, the assumption of uncorrelated errors is extremely is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.		
						
						\end{description}
						
						
						\paragraph{3. Non-constant Variance of Error Terms}
						\begin{description}
							\item[Problem:]	Another assumption of the linear regression modle is that the error terms have a constant variance, $Var(\epsilon_i) = \sigma^2$. The standard errors, confidence intervals and hypothesis tests associated with the linear model rely upon this assumption. Often it is the case that the variances of the error terms are non-constant.
							
							\item[Analysis:] One can identify non-constant variances in the errors from the presence of a funnel shape in the residual plot (Residuals versus fitted values).
							
							\item[Possible solution:] \quad
								\begin{itemize}
									\item One possible solution is to transform the response $Y$ using a concave function such as $\log Y$ or $\sqrt{Y}$.	
									\item If we have a good idea of the variance of each response, the remedy is to fit our model by weighted least squares, with weights proportional to the inverse variances.	
								\end{itemize}
						
						\end{description}
						
							
						\paragraph{4. Outliers}
						\begin{description}
							\item[Problem:]	An outlier is a point for which $y_i$ is far from the value predicted by the model. We need to identify it.	
							
							\item[Analysis:] Even if an outlier does not have much effect on the lest squares fit, it can cause other problems.
							\begin{itemize}
								\item It impacts on $RSE$. Since $RSE$ is used to compute all confidence intervals and p-values, a single point can have implications for the interpretation of the fit.
								\item Inclusion of the outlier causes the $R^2$ to decline.
							\end{itemize}
							Residual plots can be used to identify outliers, but it can be difficult to decide how large a residual needs to be before we consider the point to be an outlier. To address this problem, we can plot the studentized residuals, computed by $|e_i/\hat{SE_i}|$, if it is greater than 3 it is a possible outlier.
							
							\item[Possible solution:] simply remove the observation, however an outlier may instead indicate a deficiency with the model, such as a missing predictor.
							
						
						\end{description}
							
							
						\paragraph{5. High leverage Points}
						\begin{description}
							\item[Problem:]	They are observations with high leverage have an unusual value for $x_i$. High leverage observations tend to have a sizable impact on the estimated regression line. Any problem with these points may invalidate the entire fit.
							
							\item[Analysis:] We can look for observations for which the predictor value is outside of the normal range of the observations.
							In multiple regression model it is possible to have an observation that is well within the range of each individual predictor's value, but that is unusual in terms of the full set of predictors. The problem is more pronounced with more than two predictors.
							
							We compute the leverage statistic to quantify an observation leverage. A large value of this indicates an observation with high leverage.
							\begin{equation}
								h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{\sum_{i'=1}^n (x_{i'} - \bar{x})^2}
							\end{equation}
							Properties of equation:
							\begin{itemize}
								\item $h_i$ increase with the distance of $x_i$ from $\bar{x}$.
								\item The leverage statistic $h_i$ is always between $1/n$ and 1.
								\item The average leverage for all the observations is always equal to $(p + 1)/n$.
							\end{itemize}
							If an observation has $h_i$ that greatly exceeds $(p + 1)/n$ then we may suspect that the corresponding point has high leverage.
							If an observation has high leverage statistic and high studentized residual it is an outlier as well as a high leverage observation.
							
							\item[Possible solution:] Remove the observation.			
						
						\end{description}
							
							
						\paragraph{6. Collinearity}
						\begin{description}
							\item[Problem:] It refers to the situation in which two or more predictor variables are closely related to one another. If there collinearity it can be difficult to separate out the individual effects of collinear variables on the response.
							
							\item[Consequences:] Since collinearity reduces the accuracy of the estimates of the regression coefficients, 
								\begin{itemize}
									\item It causes the standard error for $\hat{\beta_j}$ to grow. 
									\item Consequently, collinearity results in a decline in the t-static.
									\item The power of the hypothesis test is reduced. We may fail to reject $H_0: \beta_j = 0$.
								\end{itemize}
							
							\item[Analysis:] \quad
							\begin{itemize}
								\item To detect collinearity we look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix	 (\textbf{multicollinearity}).
								\item A way to asses multicollinearity is to compute the variance inflation factor (VFI). The VIF is the ratio of the variance of $\hat{\beta_j}$ when fitting the full model divided by the variance of $\hat{\beta_j}$ if fit on its own. The smallest value of VIF is 1. A value that exceeds 5 or 10 indicates a problematic amount of collinearity.
							\end{itemize}														
														
																
							\item[Possible solution:] \quad		
							\begin{itemize}
								\item To drop one of the problematic variables from the regression.
								\item To combine the collinear variables together into a single predictor.			
							\end{itemize}								
						
						\end{description}
				
		
		
	\section{Appendix}
	
		

\end{document}
